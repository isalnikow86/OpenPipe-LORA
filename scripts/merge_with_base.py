from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import os

# ðŸ”§ Verzeichnisse (anpassen, falls du andere Namen verwendest)
base_model_path = "../base_model/Llama-3.2-3B-Instruct"
lora_path = "../lora_adapter"
output_path = "../output/merged_model"

# Modell laden
print("ðŸ”„ Lade Basis-Modell...")
model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA-Adapter laden
print("ðŸ”Œ Lade LoRA-Adapter...")
model = PeftModel.from_pretrained(model, lora_path)

# Merge durchfÃ¼hren
print("ðŸ§  Merge LoRA â†’ Base...")
model = model.merge_and_unload()

# Speichern
print("ðŸ’¾ Speichere das gemergte Modell...")
model.save_pretrained(output_path)
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.save_pretrained(output_path)

print(f"âœ… Merge abgeschlossen! Gespeichert in: {output_path}")
